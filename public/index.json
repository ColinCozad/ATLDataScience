[{"authors":["admin"],"categories":null,"content":"I am an undergraduate student at Emory University. I am currently a Technology Consultant at Emory\u0026rsquo;s Cox Computing Center. Data Analytics is my passion, and R programming is my bread and butter. I am interested in Econometrics, Data Privacy, and Business Intelligence. I believe we are in the midst of a data revolution that will define our future, meaning that we have the responsibility to shape what that future looks like through responsible data practices. When I\u0026rsquo;m not behind a computer screen, I enjoy spending time in nature and taking care of my plants. This site is maintained through the Rstats package \u0026ldquo;Blogdown\u0026rdquo; and is open-sourced on our Github Repository page.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I am an undergraduate student at Emory University. I am currently a Technology Consultant at Emory\u0026rsquo;s Cox Computing Center. Data Analytics is my passion, and R programming is my bread and butter. I am interested in Econometrics, Data Privacy, and Business Intelligence. I believe we are in the midst of a data revolution that will define our future, meaning that we have the responsibility to shape what that future looks like through responsible data practices.","tags":null,"title":"Colin Cozad","type":"author"},{"authors":["elijah"],"categories":null,"content":"Elijah is an undergraduate student from Atlanta, Georgia. After spending his freshman year at Boston University, Elijah returned to his home town of Atlanta with the goal of empowering his fellow Atlanta natives and community members. In the Fall of 2017 and Spring 2018 Elijah was a mentorship director for Project GRIND, a mentorship program for black youth in southwest Atlanta. In the fall of 2018 he started the Social Entrepreneurship Club at GSU to help students in their entrepreneurial pursuits and find ways for business to create positive social impact. Elijah is interested in using data and technology to create equitable digital communities. He is currently developing a platform for non-profit project management, collaboration and fundraising. He spends his free time monitoring financial markets, meditating, and spending time in nature.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"dff95e19c6a7571710d472530b3d6293","permalink":"/authors/elijah/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/elijah/","section":"author","summary":"Elijah is an undergraduate student from Atlanta, Georgia. After spending his freshman year at Boston University, Elijah returned to his home town of Atlanta with the goal of empowering his fellow Atlanta natives and community members. In the Fall of 2017 and Spring 2018 Elijah was a mentorship director for Project GRIND, a mentorship program for black youth in southwest Atlanta. In the fall of 2018 he started the Social Entrepreneurship Club at GSU to help students in their entrepreneurial pursuits and find ways for business to create positive social impact.","tags":null,"title":"Elijah Sullivan","type":"author"},{"authors":null,"categories":["Colin Cozad","Elijah Sullivan"],"content":" Introduction For our first joint blog post we decided to look at this week’s #TidyTuesday dataset. Tidy Tuesday is a weekly data project aimed at the R for Data Science community. Every week a raw dataset is posted so that users can apply their R skills, get feedbak, explore other’s work, and connect with the greater #RStats community.\nThis week’s data is about Board Games! The data comes from the Board Game Geek database.\nThe first step is to download the dataset from the TidyTuesday Repository.\nboard_games \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv\u0026quot;)  Building a Linear Model For this dataset we want to see if we can build a model to predict how sucessful a game’s rating might be. We begin by looking at the distribution of the average rating variable.\nboard_games %\u0026gt;% ggplot(aes(average_rating))+ geom_histogram() Average rating has an almost normal distribution which is good because it means we can apply a linear model to predict rating.\nreg1\u0026lt;-lm((average_rating) ~ log2(max_players + 1) +log2(max_playtime+1) + year_published, data = board_games) summary(reg1) ## ## Call: ## lm(formula = (average_rating) ~ log2(max_players + 1) + log2(max_playtime + ## 1) + year_published, data = board_games) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.9862 -0.4437 0.0230 0.4800 2.8934 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -4.670e+01 1.214e+00 -38.46 \u0026lt;2e-16 *** ## log2(max_players + 1) -1.951e-01 1.031e-02 -18.92 \u0026lt;2e-16 *** ## log2(max_playtime + 1) 1.664e-01 4.538e-03 36.68 \u0026lt;2e-16 *** ## year_published 2.627e-02 6.049e-04 43.44 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.748 on 10528 degrees of freedom ## Multiple R-squared: 0.2266, Adjusted R-squared: 0.2264 ## F-statistic: 1028 on 3 and 10528 DF, p-value: \u0026lt; 2.2e-16 In this model we infer how well the maximum number of players, the maximum of amount of playtime, and the year that the game was published predicts the rating. Everytime you double maximum number of players expect average rating to go down by 0.195 on average. Doubling max playtime will leads to a 0.166 increase in the average rating. \\(R^2\\) value shows that the model explains 22.6% of the data. And all the varibales are statstically significant.\n Tidyverse We noticed that the Category variable contains many values seperated by comma. We decided to use the tidyverse “separate_rows” function to seperate this variable into multiple rows, this will make it easier to analyze.\ncategorical_variables\u0026lt;- board_games %\u0026gt;% select(game_id, category) %\u0026gt;% gather(type, value, -game_id) %\u0026gt;% filter(!is.na(value)) %\u0026gt;% separate_rows(value, sep = \u0026quot;,\u0026quot;)  GGplot Visualization For this visualization we want to make a boxplot of average rating by category. This will help us understand which categories correlate with higher ratings.\nboard_games %\u0026gt;% inner_join(categorical_variables, by = c(\u0026quot;game_id\u0026quot;)) %\u0026gt;% filter(type == \u0026quot;category\u0026quot;) %\u0026gt;% mutate(value = fct_lump(value, 15), value = fct_reorder(value, average_rating)) %\u0026gt;% ggplot(aes(value, average_rating)) + geom_boxplot()+ coord_flip()+ ylim(3,9) + geom_hline(yintercept=6.37, linetype=\u0026quot;dashed\u0026quot;, color = \u0026quot;red\u0026quot;)+ #Adding a vertical line at the mean labs(y=\u0026quot;Average Rating\u0026quot;, x = \u0026quot;Board Game Category\u0026quot;, title = \u0026quot;Boxplot of Game Category and Rating\u0026quot;, caption= \u0026quot;The red dashed line represents the mean Average Rating\u0026quot;)  ","date":1552694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552694400,"objectID":"76d0cb34ae0135a43995f7ecd422c3eb","permalink":"/post/do-you-want-to-play-a-game/","publishdate":"2019-03-16T00:00:00Z","relpermalink":"/post/do-you-want-to-play-a-game/","section":"post","summary":"Introduction For our first joint blog post we decided to look at this week’s #TidyTuesday dataset. Tidy Tuesday is a weekly data project aimed at the R for Data Science community. Every week a raw dataset is posted so that users can apply their R skills, get feedbak, explore other’s work, and connect with the greater #RStats community.\nThis week’s data is about Board Games! The data comes from the Board Game Geek database.","tags":["TidyTuesday","ggplot2","tidyverse"],"title":"Do you want to play a game?","type":"post"},{"authors":["Colin Cozad"],"categories":["Colin Cozad"],"content":" Introduction Research Idea For this project I will research the racial differences and disparities in American economics, politics, and culture. I will use statistical analysis to look for correlations in these differences. The independent variable will be race and the dependent variables will be family income, political party, and average amount of TV somebody watches.\n The GSS Dataset The dataset is from the General Social Survey (GSS). It is a national survey conducted by NORC at the University of Chicago, to gather data on the complexity of American life. This data is adequate because it collects survey information about peoples preferences and lives. Thiss dataset holds information about a respondent’s age, years of education, race, family income, political party, and number of hours of TV they watch per week.\n Loading and Cleaning the Dataset GSS \u0026lt;- read_excel(\u0026quot;~/Dropbox (GPF)/School/Fall 18/Econ 220 Prob \u0026amp; Stats for Economist/Lab/GSS.xls\u0026quot;) GSS$educ \u0026lt;- as.numeric(GSS$educ) #Converting this to a numeric vector GSS$age \u0026lt;- as.numeric(GSS$age) #Converting this to a numeric vector GSS$tvhours \u0026lt;- as.numeric(GSS$tvhours) #Converting this to a numeric vector str(GSS) ## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 2869 obs. of 7 variables: ## $ year : num 2016 2016 2016 2016 2016 ... ## $ age : num 47 61 72 43 55 53 50 23 45 71 ... ## $ educ : num 16 12 16 12 18 14 14 11 12 14 ... ## $ race : chr \u0026quot;White\u0026quot; \u0026quot;White\u0026quot; \u0026quot;White\u0026quot; \u0026quot;White\u0026quot; ... ## $ partyid: chr \u0026quot;Independent\u0026quot; \u0026quot;Ind,near dem\u0026quot; \u0026quot;Not str republican\u0026quot; \u0026quot;Not str republican\u0026quot; ... ## $ tvhours: num 1 1 NA 1 NA 1 2 NA 2 NA ... ## $ coninc : num 163844 39930 59895 163844 163844 ... kable(summary(GSS), digits=2, caption=\u0026quot;Social Survey Data\u0026quot;) #Using knitr package to create a table  Table 1: Social Survey Data   year age educ race partyid tvhours coninc     Min. :2016 Min. :18.00 Min. : 0.00 Length:2869 Length:2869 Min. : 0.000 Min. : 0   1st Qu.:2016 1st Qu.:34.00 1st Qu.:12.00 Class :character Class :character 1st Qu.: 1.000 1st Qu.: 13612   Median :2016 Median :49.00 Median :13.00 Mode :character Mode :character Median : 2.000 Median : 32670   Mean :2016 Mean :48.85 Mean :13.74 NA NA Mean : 3.031 Mean : 43493   3rd Qu.:2016 3rd Qu.:62.00 3rd Qu.:16.00 NA NA 3rd Qu.: 4.000 3rd Qu.: 59895   Max. :2016 Max. :88.00 Max. :20.00 NA NA Max. :24.000 Max. :163844   NA’s :2 NA’s :34 NA’s :11 NA NA NA’s :986 NA’s :2      Graphs Graphing Income by Race GSS \u0026lt;- GSS[ which(!is.na(GSS$race) \u0026amp; !is.na(GSS$coninc)), ] #Removing NA values ggplot(data =GSS, aes(x=race, y=coninc)) + #Choosing variables race and family income geom_boxplot() + ggtitle(\u0026#39;Race and Income Boxplot\u0026#39;) + #Title of the plot ylab(\u0026quot;Family Income $\u0026quot;) + xlab(\u0026quot;Race\u0026quot;)+ #Labels for the axis theme(plot.title = element_text(hjust = 0.5)) #Centering the title This boxplot displays the distribution of income by race. This has profuond insights, such as the fact that Black people have the lowest mean income, with “other” having the middle average, and White people having the highest income.\n Graphing Race, Age, and TV Hours gss_cat %\u0026gt;% group_by(race, age) %\u0026gt;% summarise(meantv=mean(tvhours, na.rm = T)) %\u0026gt;% #Using dplyr to create a mean TV hours variable and summarise it by race and age ggplot(aes(x=age, y=meantv,color=race))+ #GGplot x-axis is age, y-axis is Mean TV Hours, and Color is race geom_point() + #Speceifying a scatterplot geom_smooth(method = \u0026quot;lm\u0026quot;)+ #Affs a trend line on the plot ylab(\u0026quot;Hours of TV per Week\u0026quot;) + xlab(\u0026quot;Age\u0026quot;)+ #Labels for the axis ggtitle(\u0026quot;Age, Hours of TV, and Race Scatterplot\u0026quot;)+ #Title of the plot theme(plot.title = element_text(hjust = 0.5)) #Centering the title The graph shows that for White, Black, and other races the hours of TV watched per week rises as that population gets older. The three seperate trend lines also reveal racial differences. Whites and others watch similiar amounts of TV especially as they get older, Black people watched signifcantly more TV on average than the other races.\n Graphing Race and Political Party GSS$partyid\u0026lt;-ifelse(GSS$partyid == \u0026quot;Strong republican\u0026quot;, \u0026quot;Republican\u0026quot;,ifelse(GSS$partyid == \u0026quot;Not str republican\u0026quot;, \u0026quot;Republican\u0026quot;, ifelse(GSS$partyid == \u0026quot;Ind, near rep\u0026quot;, \u0026quot;Republican\u0026quot;, ifelse(GSS$partyid == \u0026quot;Ind, near dem\u0026quot;, \u0026quot;Democrat\u0026quot;, ifelse(GSS$partyid == \u0026quot;Not str democrat\u0026quot;, \u0026quot;Democrat\u0026quot;,ifelse(GSS$partyid == \u0026quot;Strong Democrat\u0026quot;, \u0026quot;Democrat\u0026quot;, \u0026quot;Independent\u0026quot; )))))) #Using an ifelse ladder to combine political party identification into Democrat, Repubican, Independent ggplot(GSS, aes(partyid, fill = race)) + #Selecting political party as variable and race as group value theme(axis.text.x = element_text(angle = 45, hjust = 1))+ #This line tilts the axis names so they are readable geom_bar(position = \u0026quot;dodge\u0026quot;)+ #This splits the bar chart by race instead of stacking the bars geom_text(aes(label=..count..),stat=\u0026quot;count\u0026quot;,position=position_dodge(width=0.9), vjust=-0.25)+ #Adds labels to each bar ylab(\u0026quot;Count\u0026quot;) + xlab(\u0026quot;Political Party\u0026quot;)+ #Labels for the axis ggtitle(\u0026quot;Race and Political Party Bar Chart\u0026quot;)+ #Title of the plot theme(plot.title = element_text(hjust = 0.5)) #Centering the title This graph reveals that Democrats are the most racially diverse political group, while Independent is the most popular political category.\n  Analysis Income mean_inc \u0026lt;- aggregate(coninc~race, data=GSS, mean) #Use aggregate function to take mean of family income for each race print(mean_inc) ## race coninc ## 1 Black 28445.62 ## 2 Other 38010.33 ## 3 White 47727.90 This information shows that the mean income for the White race is 1.7 times more than the mean income for the Black race, clearly there is a racial income disparity in the United States.\n TV Hours One of the most evident correlations was between TV Hours and Age. This relationship can further be explored by running a regression on the two variables.\ntvhours_reg\u0026lt;-lm(tvhours~age + coninc, data=GSS) #Running a linear regression on TV hours and Age print(tvhours_reg) #Pritning results of the regression ## ## Call: ## lm(formula = tvhours ~ age + coninc, data = GSS) ## ## Coefficients: ## (Intercept) age coninc ## 1.647e+00 4.067e-02 -1.371e-05 summary(tvhours_reg) #Summarising the regression ## ## Call: ## lm(formula = tvhours ~ age + coninc, data = GSS) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.9414 -1.4945 -0.5185 0.8314 20.3600 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.647e+00 1.929e-01 8.541 \u0026lt;2e-16 *** ## age 4.067e-02 3.485e-03 11.669 \u0026lt;2e-16 *** ## coninc -1.371e-05 1.480e-06 -9.267 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.665 on 1858 degrees of freedom ## (1006 observations deleted due to missingness) ## Multiple R-squared: 0.1079, Adjusted R-squared: 0.1069 ## F-statistic: 112.3 on 2 and 1858 DF, p-value: \u0026lt; 2.2e-16 mean_tv \u0026lt;- aggregate(tvhours~race, data=GSS, mean) #Use Aggregate to find mean TV Hours per Race print(mean_tv) ## race tvhours ## 1 Black 4.046875 ## 2 Other 2.697297 ## 3 White 2.840348 The regression allows us to see the relationship between age and hours of TV one watches. The regression shows that an increase in age by 1 year results in a 0.04 increase in the hours of TV someone watches. An increase in income by 1,000 is associated with 0.015 decrease in the amount of TV one watches. However; the R-squared value shows that these variables only explain 10% of the data observations. From this data we can also compare the mean hours of TV each race watches.\n  Inferences From the social survey data one can analyze racial differences within the United States. A boxplot was used to compare the Family Income averages across races. This revealed that black people have the lowest average wages and speaks to a larger culture of income inequality within the United States. These economic differences effected the racial makeup of America’s political parties. Democrats were the most racially diverse, while Independent was the most popular. After looking at economic and political differences we anlayzed the niche cultural difference of hours of TV watched. Across all races hours of TV increased with age but when split by race it was clear that blakc people watch more TV than the other races. This anlaysis helps understand how one’s race is likely to correlate with economic, political, and cultural differences in the United States.\n ","date":1543881600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543881600,"objectID":"0eba5a9823a2e1aa0e2346538c279224","permalink":"/post/gss-data-project/","publishdate":"2018-12-04T00:00:00Z","relpermalink":"/post/gss-data-project/","section":"post","summary":"Introduction Research Idea For this project I will research the racial differences and disparities in American economics, politics, and culture. I will use statistical analysis to look for correlations in these differences. The independent variable will be race and the dependent variables will be family income, political party, and average amount of TV somebody watches.\n The GSS Dataset The dataset is from the General Social Survey (GSS). It is a national survey conducted by NORC at the University of Chicago, to gather data on the complexity of American life.","tags":["R Markdown","ggplot2","tidyverse","Regression"],"title":"Exploring US Demographics with data from the General Social Survey","type":"post"},{"authors":["Colin Cozad"],"categories":["Colin Cozad"],"content":" Exploratory Data Analysis with the Diamonds Dataset For this post I decided to walk through the steps of exploratory data analysis using the diamonds dataset that come with base R.\nPlotting the Data data(\u0026#39;diamonds\u0026#39;) #Loading the dataset summary(diamonds) #Looking at the summary of the dataset ## carat cut color clarity ## Min. :0.2000 Fair : 1610 D: 6775 SI1 :13065 ## 1st Qu.:0.4000 Good : 4906 E: 9797 VS2 :12258 ## Median :0.7000 Very Good:12082 F: 9542 SI2 : 9194 ## Mean :0.7979 Premium :13791 G:11292 VS1 : 8171 ## 3rd Qu.:1.0400 Ideal :21551 H: 8304 VVS2 : 5066 ## Max. :5.0100 I: 5422 VVS1 : 3655 ## J: 2808 (Other): 2531 ## depth table price x ## Min. :43.00 Min. :43.00 Min. : 326 Min. : 0.000 ## 1st Qu.:61.00 1st Qu.:56.00 1st Qu.: 950 1st Qu.: 4.710 ## Median :61.80 Median :57.00 Median : 2401 Median : 5.700 ## Mean :61.75 Mean :57.46 Mean : 3933 Mean : 5.731 ## 3rd Qu.:62.50 3rd Qu.:59.00 3rd Qu.: 5324 3rd Qu.: 6.540 ## Max. :79.00 Max. :95.00 Max. :18823 Max. :10.740 ## ## y z ## Min. : 0.000 Min. : 0.000 ## 1st Qu.: 4.720 1st Qu.: 2.910 ## Median : 5.710 Median : 3.530 ## Mean : 5.735 Mean : 3.539 ## 3rd Qu.: 6.540 3rd Qu.: 4.040 ## Max. :58.900 Max. :31.800 ##  From the summary I notice that this dataset features different variables about diamonds. I notice that there is a numeric “price” variable and a categorical “cut”\u0026quot; variable. I wonder if there is a relationship between the price of the diamond and the cut, and if the cut can be used to predict price?\nggplot(diamonds, aes(x=cut, y = price))+ geom_boxplot()+ ggtitle(\u0026quot;Comparing Diamond Cut and Price\u0026quot;) I decided to do a boxplot that measures the price statstics for each cut. The results were unexpected, fair diamonds had the highest average price while ideal had the lowest. There seems to be a lot of outliers in this visualization.\nggplot(diamonds, aes(x=price))+ geom_histogram()+ ggtitle(\u0026quot;Diamond Price Histogram\u0026quot;) I decided to make a histogram of diamond price to get a better understanding of the first boxplot. This histogram has a strong right skew, meaning that the vast majority of diamonds are sold on the lower end of the price range.\nggplot(data = diamonds, aes(x = price)) + geom_histogram() + facet_wrap(~ cut)+ ggtitle(\u0026quot;Diamond Price Histograms by Cut\u0026quot;) I decided to recreate the same histogram but this time facetting the plot by cut. This histogram helps to explain the unexpected results in the first boxplot. The reason why fair diamonds have the highest average price while ideal diamonds have the lowest has to do with quantity. There are far more ideal diamonds than fair. This also explains the large number of outliers in the boxplot. The cut of the diamond cannot be used to predict price. Perhaps the carat would have a better linear relationship with price? To explore this I will create a scatterplot of the carat and price.\nggplot(diamonds, aes(x= carat,y = price))+ geom_point()+ geom_smooth(color= \u0026quot;red\u0026quot;, se=FALSE)+ ggtitle(\u0026quot;Diamond Carat and Price Scatter Plot\u0026quot;) While there are still many outliers there is a stronger linear relationship between carat and price than cut and price.\n Results The diamonds dataset contains information about the price and quality of diamonds. The data shows that the price of diamonds is not correlated with the cut of the diamond. After looking at a histogram it is apparent that this is due to skew in the frequency of the diamonds by cut. There are significantly more Premium and Ideal diamonds than Fair and Good. This is an interesting result that one would not expect. It turns out that the carat of the diamond is a better predictor of price. As the carat goes up so does the price. However; there are not many diamonds that are over three carats in the data. Anyone familiar with diamonds would expect a higher carat to be associated with a higher quality and price, this data supports this expectation.\n  ","date":1538611200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538611200,"objectID":"86abcc791d45b260697966582598e054","permalink":"/post/eda-project/","publishdate":"2018-10-04T00:00:00Z","relpermalink":"/post/eda-project/","section":"post","summary":"Exploratory Data Analysis with the Diamonds Dataset For this post I decided to walk through the steps of exploratory data analysis using the diamonds dataset that come with base R.\nPlotting the Data data(\u0026#39;diamonds\u0026#39;) #Loading the dataset summary(diamonds) #Looking at the summary of the dataset ## carat cut color clarity ## Min. :0.2000 Fair : 1610 D: 6775 SI1 :13065 ## 1st Qu.:0.4000 Good : 4906 E: 9797 VS2 :12258 ## Median :0.","tags":["R Markdown","ggplot2","tidyverse"],"title":"Exploratory Data Analysis with Diamonds","type":"post"},{"authors":null,"categories":null,"content":"","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":null,"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461729600,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00-04:00","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":null,"title":"Internal Project","type":"project"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441080000,"objectID":"d77fa4a74076ffcd7ca6c21cfc27a4b2","permalink":"/publication/person-re-id/","publishdate":"2015-09-01T00:00:00-04:00","relpermalink":"/publication/person-re-id/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372651200,"objectID":"2b4d919e3cf73dfcd0063c88fe01cb00","permalink":"/publication/clothing-search/","publishdate":"2013-07-01T00:00:00-04:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"A mobile visual clothing search system is presented whereby a smart phone user can either choose a social networking image or capture a new photo of a person wearing clothing of interest and search for similar clothing in a large cloud-based ecommerce database. The phone's GPS location is used to re-rank results by retail store location, to inform the user of local stores where similar clothing items can be tried on.","tags":[],"title":"Mobile visual clothing search","type":"publication"}]